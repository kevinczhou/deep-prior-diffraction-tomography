{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from DT import DT\n",
    "from time import time\n",
    "import scipy.io\n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define settings for optimization\n",
    "Define experimental hyperparameters (e.g., scattering model, regularization settings) for the optimization run as a Python dictionary. Below are the settings used for the experimental results in the paper (Figs. 7-10) -- choose one of these or create your own. Reduce batch size if your GPU runs out of memory (the batch sizes specified below should work for a 16-GB GPU). \n",
    "\n",
    "```python\n",
    "# 2-um bead sample settings using first Born model (Fig. 7):\n",
    "experiment = {'sample': '2umbeads', 'model': 'born',\n",
    "              'reg': 'None', 'num_iter': 500, 'batch_size': 961}\n",
    "experiment = {'sample': '2umbeads', 'model': 'born',\n",
    "              'reg': 'DIP', 'activation': 'leaky', 'num_iter': 20000, 'batch_size': 961}\n",
    "experiment = {'sample': '2umbeads', 'model': 'born',\n",
    "              'reg': 'positivity', 'coeff': 1e-3, 'num_iter': 500, 'batch_size': 961}\n",
    "experiment = {'sample': '2umbeads', 'model': 'born',\n",
    "              'reg': 'TV', 'coeff': 1e-7, 'num_iter': 500, 'batch_size': 961}\n",
    "\n",
    "# 1-um bead (actually 800-nm) sample settings using first Born model (Fig. 8):\n",
    "experiment = {'sample': '1umbeads', 'model': 'born',\n",
    "              'reg': 'None', 'num_iter': 500, 'batch_size': 961}\n",
    "experiment = {'sample': '1umbeads', 'model': 'born',\n",
    "              'reg': 'DIP', 'activation': 'linear', 'num_iter': 20000, 'batch_size': 961}\n",
    "experiment = {'sample': '1umbeads', 'model': 'born',\n",
    "              'reg': 'negativity', 'coeff': 1e-3, 'num_iter': 500, 'batch_size': 961}\n",
    "experiment = {'sample': '1umbeads', 'model': 'born',\n",
    "              'reg': 'TV', 'coeff': 1e-7, 'num_iter': 500, 'batch_size': 961}\n",
    "\n",
    "# starfish sample settings using first Born model (Fig. 9):\n",
    "experiment = {'sample': 'starfish', 'model': 'born',\n",
    "              'reg': 'None', 'num_iter': 500, 'batch_size': 481}\n",
    "experiment = {'sample': 'starfish', 'model': 'born',\n",
    "              'reg': 'DIP', 'activation': 'linear', 'num_iter': 10000, 'batch_size': 481}\n",
    "experiment = {'sample': 'starfish', 'model': 'born',\n",
    "              'reg': 'positivity', 'coeff': 1e-2, 'num_iter': 500, 'batch_size': 481}\n",
    "experiment = {'sample': 'starfish', 'model': 'born',\n",
    "              'reg': 'TV', 'coeff': 1e-7, 'num_iter': 500, 'batch_size': 481}\n",
    "\n",
    "# 2-um bead sample settings using multislice model (Fig. 10):\n",
    "experiment = {'sample': '2umbeads', 'model': 'multislice',\n",
    "              'reg': 'None', 'num_iter': 5000, 'batch_size': 961}\n",
    "experiment = {'sample': '2umbeads', 'model': 'multislice',\n",
    "              'reg': 'DIP', 'activation': 'leaky', 'num_iter': 60000, 'batch_size': 481}\n",
    "experiment = {'sample': '2umbeads', 'model': 'multislice',\n",
    "              'reg': 'positivity', 'coeff': 1e-3, 'num_iter': 5000, 'batch_size': 961}\n",
    "experiment = {'sample': '2umbeads', 'model': 'multislice',\n",
    "              'reg': 'TV', 'coeff': 1e-7, 'num_iter': 30000, 'batch_size': 961}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for the optimization run (create your own or use one of the above):\n",
    "experiment = {'sample': 'starfish', 'model': 'born', 'reg': 'None', 'num_iter': 500, 'batch_size': 481}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load multi-angle dataset\n",
    "The raw data includes the full 500x500-pixel field of view (for 961 LEDs), but only a reduced field of view is reconstructed, depending on the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data from DPDT_raw_data folder:\n",
    "if '2um' in experiment['sample']:\n",
    "    path = './DPDT_raw_data/2umbead2layer.mat'\n",
    "elif '1um' in experiment['sample']:\n",
    "    path = './DPDT_raw_data/1umbead.mat'\n",
    "elif 'star' in experiment['sample']:\n",
    "    path = './DPDT_raw_data/starfish.mat'\n",
    "\n",
    "stack = scipy.io.loadmat(path)\n",
    "stack = stack['hdrarray_out'].astype(np.float32)\n",
    "stack = np.transpose(stack, (2, 0, 1))\n",
    "stack = stack[None, :, None]\n",
    "\n",
    "# reduce the field of view:\n",
    "if 'born' in experiment['model']:\n",
    "    if '1um' in experiment['sample']:\n",
    "        stack = stack[..., 200:363, 0:163]\n",
    "    elif '2um' in experiment['sample']:\n",
    "        stack = stack[..., 240:403, 20:183]\n",
    "    elif 'star' in experiment['sample']:\n",
    "        stack = stack[..., 0:450, 20:470]\n",
    "else:\n",
    "    stack = stack[..., 240:240+256, :256]\n",
    "\n",
    "# plot example image:\n",
    "plt.imshow(stack[0, 480, 0])\n",
    "plt.title('center LED image')\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tensorflow graph based on specified settings\n",
    "See the DT.py file for more detailed explanations of the various settings modified below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'born' in experiment['model']:\n",
    "    if 'starfish' in experiment['sample']:\n",
    "        a = DT(im_size=stack.shape[-1], xy_upsamp=1, z_upsamp=2, z_fov_upsamp=0.5)\n",
    "        a.coordinate_offset = np.array([0, 0, 0], dtype=np.float32)\n",
    "    elif '1um' in experiment['sample']:\n",
    "        a = DT(im_size=stack.shape[-1], xy_upsamp=2, z_upsamp=4, z_fov_upsamp=1)\n",
    "        a.coordinate_offset = np.array([1, 1, 0], dtype=np.float32)\n",
    "    elif '2um' in experiment['sample']:\n",
    "        a = DT(im_size=stack.shape[-1], xy_upsamp=1, z_upsamp=4, z_fov_upsamp=1)\n",
    "        a.coordinate_offset = np.array([.5, .5, 0], dtype=np.float32)\n",
    "    else:\n",
    "        raise Exception('invalid sample')\n",
    "\n",
    "    a.force_pass_thru_DC = True\n",
    "    a.train_DC = True\n",
    "        \n",
    "    if 'DIP' in experiment['reg']:\n",
    "        a.optimize_k_directly = False\n",
    "    else:\n",
    "        a.optimize_k_directly = True\n",
    "\n",
    "if 'multislice' in experiment['model']:\n",
    "    # here, the spatial patch size is 128x128, but the full reconstruction size is 256x256\n",
    "    a = DT(im_size=128, im_size_full=256, scattering_model='multislice', xy_upsamp=1,\n",
    "            sample_thickness_MS=10, sample_pix_MS=32, use_spatial_patching=True)\n",
    "\n",
    "    a.force_pass_thru_DC = False\n",
    "    a.train_DC = False\n",
    "    a.focus_init = -3\n",
    "    a.optimize_k_directly = False       \n",
    "\n",
    "# set regularization hyperparameters:\n",
    "if 'DIP' in experiment['reg']:\n",
    "    a.use_deep_image_prior = True\n",
    "    if experiment['activation'] == 'leaky':\n",
    "        a.linear_DIP_output = False\n",
    "    elif experiment['activation'] == 'linear':\n",
    "        a.linear_DIP_output = True\n",
    "    else:\n",
    "        raise Exception('invalid activation')\n",
    "else:\n",
    "    a.use_deep_image_prior = False\n",
    "    \n",
    "if 'TV' in experiment['reg']:\n",
    "    a.TV_reg_coeff = experiment['coeff']\n",
    "else:\n",
    "    a.TV_reg_coeff = 0\n",
    "    \n",
    "if 'positivity' in experiment['reg']:\n",
    "    a.positivity_reg_coeff = experiment['coeff']\n",
    "else:\n",
    "    a.positivity_reg_coeff = 0\n",
    "    \n",
    "if 'negativity' in experiment['reg']:\n",
    "    a.negativity_reg_coeff = experiment['coeff']\n",
    "else:\n",
    "    a.negativity_reg_coeff = 0\n",
    "    \n",
    "a.batch_size = experiment['batch_size']\n",
    "    \n",
    "# for ignoring dark field LEDs:\n",
    "x = np.linspace(-1, 1, 31)\n",
    "x, y = np.meshgrid(x, x)\n",
    "background = x**2 + y**2 < .9\n",
    "background = background.flatten()\n",
    "a.data_ignore = ~background\n",
    "\n",
    "# create graph and session for optimzation:\n",
    "median = np.median(stack, (0, 2, 3, 4))\n",
    "a.format_DT_data(stack / median[None, :, None, None, None], DC=np.ones_like(median))\n",
    "a.reconstruct()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize\n",
    "Run the optimization loop, monitoring progress periodically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = list()  # to store the loss history\n",
    "feed = {}  # feed_dict for the gradient step; may be used to modify learning rate for DIP\n",
    "num_iter = experiment['num_iter']  # number of gradient steps\n",
    "plot_iter = num_iter // 10  # plot cross-sections of the reconstruction every plot_iter iterations\n",
    "\n",
    "for i in range(num_iter):\n",
    "\n",
    "    start = time()\n",
    "    _, loss_i = sess.run([a.train_op, a.loss], feed_dict=feed)\n",
    "    losses.append(loss_i)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(i, loss_i, time()-start)\n",
    "        # loss_i is a list of all the loss terms, with the first being the data-dependent loss\n",
    "\n",
    "    # monitor results periodically:\n",
    "    if i % plot_iter == 0 or i == num_iter-1:\n",
    "        RI = a.RI.eval()\n",
    "\n",
    "        # plot an xz and xy cross section:\n",
    "        plt.figure(figsize=(15,8))\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(np.real(RI[19].T))\n",
    "        plt.title('RI (xz slice)')\n",
    "        plt.colorbar()\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(np.real(RI[:, :, a.side_kz//2 + 2]))\n",
    "        plt.title('RI (xy slice)')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "\n",
    "        # plot loss curve on linear and log scale;\n",
    "        # the data-dependent term is separated from the regularization terms in this plot;\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.subplot(121)\n",
    "        plt.plot(losses)\n",
    "        plt.title('loss')\n",
    "        plt.subplot(122)\n",
    "        plt.plot(np.log(losses))\n",
    "        plt.title('log loss')\n",
    "        plt.legend(['data-dependent loss'])\n",
    "        plt.show()\n",
    "\n",
    "    # roll back if DIP optimization diverges and anneal learning rate (only for first Born model):\n",
    "    save_iter = 15\n",
    "    if a.use_deep_image_prior and not a.scattering_model == 'multislice':\n",
    "        # average the last few losses (only the data loss portion):\n",
    "        last_few_DIP_losses = [loss[0] for loss in losses[-6:-1]]\n",
    "        if i > 500 and losses[-1][0] > 4 * np.mean(last_few_DIP_losses):\n",
    "            print('Optimization diverged; rolling back ...')\n",
    "            # don't restore the last one, but the last last one:\n",
    "            if last_saved == 'model':\n",
    "                a.saver.restore(sess, '/tmp/model_.ckpt')\n",
    "            elif last_saved == 'model_':\n",
    "                a.saver.restore(sess, '/tmp/model.ckpt')\n",
    "            feed[a.DIP_lr] *= .9  # anneal learning rate\n",
    "        elif i % (save_iter * 2) == 0:\n",
    "            a.saver.save(sess, '/tmp/model.ckpt')\n",
    "            last_saved = 'model'\n",
    "        elif i % save_iter == 0:\n",
    "            # save 2 checkpoints to be sure to roll back at least save_iter iterations\n",
    "            a.saver.save(sess, '/tmp/model_.ckpt')\n",
    "            last_saved = 'model_'\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra step for DIP reconstructions using the mutislice model and spatial patching\n",
    "It is too computationally intensive for the DIP to generate the whole reconstruction field of view. Instead, we use \"spatial patching\" where at every iteration only a small, randomly chosen patch within the full field of view contributes to the loss. The DIP output matches the patch size and uses a common network for all spatial patches. To reconstruct the whole field of view, we use our stochastic stitching algorithm, which involves selecting many patches randomly and stitching them together.\n",
    "\n",
    "Using our spatial patching scheme, we can theoretically reconstruct an arbitrarily-sized reconstruction seamlessly with a fixed memory budget. For more details, see the appendix of our paper, which is listed in the README file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'DIP' in experiment['reg'] and 'multislice' in experiment['model']:\n",
    "    # create full-size reconstruction by stitching 1000 patches:\n",
    "    RI_stochastic_stitch = a.stochastic_stitch(sess, depad=35)\n",
    "    \n",
    "    # plot cross sections of the full 3D reconstruction:\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(np.real(RI_stochastic_stitch[:, 19].T))\n",
    "    plt.title('RI, full FOV (xz slice)')\n",
    "    plt.colorbar()\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(np.real(RI_stochastic_stitch[:, :, a.side_kz//2 + 2]))\n",
    "    plt.title('RI, full FOV (xy slice)')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1",
   "language": "python",
   "name": "tf1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
